
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><meta content="Use a classical recurrent neural network to initilize the parameters of a variational quatum algorithm." property="og:description" />
<meta content="../demonstrations/learning2learn/thumbnail.png" property="og:image" />

  

  <meta property="og:title" content="Learning to learn with quantum neural networks &#8212; PennyLane">
  <meta property="og:url" content="https://pennylane.ai/qml/demos/learning2learn.html">
  <meta name="twitter:card" content="summary_large_image">

  
  
  <meta content="Use a classical recurrent neural network to initilize the parameters of a variational quatum algorithm." name="description" />
  

  <link href="https://fonts.googleapis.com/css?family=Noto+Serif" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Noto+Sans" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css">
  <!-- Bootstrap core CSS -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/css/bootstrap.min.css" rel="stylesheet">

  <!-- Material Design Bootstrap -->
  <!-- <link href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.8.10/css/mdb.min.css" rel="stylesheet"> -->

  <!-- nanoscroller -->
  <link rel="stylesheet" type="text/css" href="../_static/css/nanoscroller.css" />

  <!-- lightslider -->
  <link type="text/css" rel="stylesheet" href="../_static/css/lightslider.min.css" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       "HTML-CSS": { scale: 90, linebreaks: { automatic: true } },
       SVG: { linebreaks: { automatic: true } },
       TeX: {
         Macros: {
           pr : ['|\#1\\rangle\\langle\#1|',1],
           ket: ['\\left| \#1\\right\\rangle',1],
           bra: ['\\left\\langle \#1\\right|',1],
           xket: ['\\left| \#1\\right\\rangle_x',1],
           xbra: ['\\left\\langle \#1\\right|_x',1],
           braket: ['\\langle \#1 \\rangle',1],
           braketD: ['\\langle \#1 \\mid \#2 \\rangle',2],
           braketT: ['\\langle \#1 \\mid \#2 \\mid \#3 \\rangle',3],
           ketbra: ['| #1 \\rangle \\langle #2 |',2],
           hc: ['\\text{h.c.}',0],
           cc: ['\\text{c.c.}',0],
           h: ['\\hat',0],
           nn: ['\\nonumber',0],
           di: ['\\frac{d}{d \#1}',1],
           bm: ['\\mathbf{\#1}',1],
           uu: ['\\mathcal{U}',0],
           inn: ['\\text{in}',0],
           out: ['\\text{out}',0],
           vac: ['\\text{vac}',0],
           I: ['I',0],
           x: ['\\hat{x}',0],
           p: ['\\hat{p}',0],
           a: ['\\hat{a}',0],
           ad: ['\\hat{a}^\\dagger',0],
           n: ['\\hat{n}',0],
           nbar: ['\\overline{n}',0],
           sech: ['\\mathrm{sech~}',0],
           tanh: ['\\mathrm{tanh~}',0],
           re: ['\\text{Re}',0],
           im: ['\\text{Im}',0],
           tr: ['\\mathrm{Tr} #1',1],
           diag: ['\\mathrm{diag} #1',1],
           sign: ['\\text{sign}',0],
           overlr: ['\\overset\\leftrightarrow{\#1}',1],
           overl: ['\\overset\leftarrow{\#1}',1],
           overr: ['\\overset\rightarrow{\#1}',1],
           avg: ['\\left< \#1 \\right>',1],
           slashed: ['\\cancel{\#1}',1],
           bold: ['\\boldsymbol{\#1}',1],
           d: ['\\mathrm d',0]
         }
       }
     });
     </script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-130507810-1"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-130507810-1');
      </script>

  <title>Learning to learn with quantum neural networks &#8212; PennyLane</title>
  
    <link rel="stylesheet" href="../_static/xanadu.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/xanadu_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-rendered-html.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <link rel="canonical" href="https://pennylane.ai/qml/demos/learning2learn.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Quanvolutional Neural Networks" href="tutorial_quanvolution.html" />
    <link rel="prev" title="The Quantum Graph Recurrent Neural Network" href="tutorial_qgrnn.html" /> 
  </head><body><link rel="stylesheet" type="text/css" href="../_static/xanadu_gallery.css" />
  <!--Navbar-->
<nav class="navbar navbar-expand-lg navbar-light white sticky-top">

  <!-- Navbar brand -->
  <a class="navbar-brand" href="https://pennylane.ai">
    <img class="pr-1" src="../_static/xanadu_x.png" width="28px"></img>
    <img src="../_static/pennylane.png" width="180px"></img>
  </a>

  <!-- Collapse button -->
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#basicExampleNav"
    aria-controls="basicExampleNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <!-- Collapsible content -->
  <div class="collapse navbar-collapse" id="basicExampleNav">

    <!-- Links -->
    <ul class="navbar-nav mr-auto">
      <li class="nav-item active">
        <a class="nav-link" href="https://pennylane.ai/qml">Quantum machine learning
          <span class="sr-only">(current)</span>
        </a>
      </li>
      <li class="nav-item">
        <a href="https://pennylane.ai/qml/demonstrations.html" class="nav-link">Demos</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="https://pennylane.ai/install.html">Install</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="https://pennylane.ai/plugins.html">Plugins</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="https://pennylane.readthedocs.io">Documentation</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="https://pennylane.ai/blog">Blog</a>
      </li>
      <li class="nav-item">
        <a class="nav-link q-hack-link" href="https://qhack.ai">
          <img src="https://pennylane.ai/img/qhack_plain_black.png">
        </a>
      </li>
    </ul>
    <!-- Links -->

    <ul class="navbar-nav ml-auto nav-flex-icons">
      <li class="nav-item">
        <a class="nav-link" href="http://pennylane.ai/faq.html">
          <i class="fab fas fa-question"></i> FAQ
        </a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="https://discuss.pennylane.ai">
          <i class="fab fab fa-discourse"></i> Support
        </a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="https://github.com/XanaduAI/PennyLane">
          <i class="fab fa-github"></i> GitHub
        </a>
      </li>
    </ul>
  </div>
  <!-- Collapsible content -->
</nav>
<!--/.Navbar-->

<script type="text/javascript">
var isDemoPages = window.location.pathname.includes('/demos_') || window.location.pathname.includes('/demonstrations') || window.location.pathname.includes('/demos/');

if (isDemoPages) {
  var $navItems = $('.nav-item');

  var previousActiveLink = $navItems.filter(function(index, item) {
    var $item = $(item);
    return $item.hasClass('active') && $item.innerText !== 'Demos';
  });

  if (previousActiveLink.length) $(previousActiveLink[0]).removeClass('active');

  var demoLink = $navItems.filter(function(index, item) {
    return item.innerText === 'Demos';
  });

  if (demoLink.length) $(demoLink[0]).addClass('active');
}
</script>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="tutorial_quanvolution.html" title="Quanvolutional Neural Networks"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="tutorial_qgrnn.html" title="The Quantum Graph Recurrent Neural Network"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">PennyLane  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../demonstrations.html" >Demos</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../demos_qml.html" accesskey="U">Quantum machine learning</a> &#187;</li> 
      </ul>
    </div>
        <div id="content">


          <div id="right-column">
            <div class="document clearer body">

              <div class="container-wrapper">

                <div role="navigation" aria-label="breadcrumbs navigation">
                  <ol class="breadcrumb">
                  </ol>
                </div>

              
  <div class="sphx-glr-download-link-note admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Click <a class="reference internal" href="#sphx-glr-download-demos-learning2learn-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="learning-to-learn-with-quantum-neural-networks">
<span id="sphx-glr-demos-learning2learn-py"></span><h1>Learning to learn with quantum neural networks<a class="headerlink" href="#learning-to-learn-with-quantum-neural-networks" title="Permalink to this headline">¶</a></h1>
<p><script type="text/javascript">
    var related_tutorials = ["tutorial_qaoa_intro.html", "tutorial_qaoa_maxcut.html"];
    var related_tutorials_titles = ['QAOA', 'QAOA for MaxCut problem'];
</script></p>
<p><em>Author: Stefano Mangini (mangini.stfn&#64;gmail.com). Posted: 2 March 2021. Last updated: 15 Sep 2021.</em></p>
<p>In this demo we recreate the architecture proposed
in <em>Learning to learn with quantum neural networks via
classical neural networks</em> <a class="footnote-reference" href="#l2l" id="id1">[1]</a>, using <strong>PennyLane</strong> and <strong>TensorFlow</strong>.
We use classical recurrent neural networks to assist
the optimization of variational quantum algorithms.</p>
<p>We start with a brief theoretical overview explaining the problem
and the setup used to solve it. After that, we deep dive into the
code to build a fully functioning model, ready to be further developed
or customized for your own needs. Without further ado, let’s begin!</p>
<div class="section" id="problem-optimization-of-variational-quantum-algorithms">
<h2>Problem: Optimization of Variational Quantum Algorithms<a class="headerlink" href="#problem-optimization-of-variational-quantum-algorithms" title="Permalink to this headline">¶</a></h2>
<p>Recently, a big effort by the quantum computing community has been
devoted to the study of variational quantum algorithms (VQAs)
which leverage quantum circuits with fixed shape and tunable
parameters. The idea is similar to
classical neural networks, where the weights of the network are
optimized during training. Similarly, once the shape of the variational quantum
circuit is chosen — something that is very difficult and sensitive to
the particular task at hand — its tunable parameters are optimized
iteratively by minimizing a cost (or loss) function, which measures how
good the quantum algorithm is performing (see <a class="footnote-reference" href="#vqas" id="id2">[2]</a> for a
thorough overview on VQAs).</p>
<p>A major challenge for VQAs relates to the optimization of tunable
parameters, which was shown to be a very hard task <a class="footnote-reference" href="#barren" id="id3">[3]</a>, <a class="footnote-reference" href="#vqas" id="id4">[2]</a> .
Parameter initialization plays a key role in this scenario,
since initializing the parameters in the proximity of an optimal
solution leads to faster convergence and better results. Thus, a good initialization
strategy is crucial to promote the convergence of local optimizers to
local extrema and to select reasonably good local minima. By local
optimizer, we mean a procedure that moves from one solution to another
by small (local) changes in parameter space. These are opposed to global
search methods, which take into account large sections of
parameter space to propose a new solution.</p>
<p>One such strategy could come from the classical machine learning
literature.</p>
</div>
<div class="section" id="solution-classical-recurrent-neural-networks">
<h2>Solution: Classical Recurrent Neural Networks<a class="headerlink" href="#solution-classical-recurrent-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>By building on results from the <em>meta-learning</em> literature in machine learning,
authors in <a class="footnote-reference" href="#l2l" id="id5">[1]</a> propose to use a Recurrent Neural Network (RNN)
as a black-box controller to optimize the parameters of
variational quantum algorithms, as shown in the figure below. The cost
function used is the expectation value <span class="math notranslate nohighlight">\(\langle H \rangle_{\boldsymbol{\theta}} = \langle \psi_{\boldsymbol{\theta}} | H | \psi_{\boldsymbol{\theta}}\rangle\)</span>
of a Hamiltonian <span class="math notranslate nohighlight">\(H\)</span> with respect to the parametrized state
<span class="math notranslate nohighlight">\(|\psi_\boldsymbol{\theta}\rangle\)</span> evolved by applying the variational quantum circuit to the zero state <span class="math notranslate nohighlight">\(|00\cdots0\rangle\)</span>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/HybridLSTM.png"><img alt="../_images/HybridLSTM.png" src="../_images/HybridLSTM.png" style="width: 100%;" /></a>
</div>
<p>Given parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{t-1}\)</span> of the variational quantum circuit,
the cost function <span class="math notranslate nohighlight">\(y_{t-1}\)</span>, and the hidden state of the
classical network <span class="math notranslate nohighlight">\(\boldsymbol{h}_{t-1}\)</span> at the previous time step, the
recurrent neural network proposes a new
guess for the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_t\)</span>, which are
then fed into the quantum computer to evaluate the
cost function <span class="math notranslate nohighlight">\(y_t\)</span>. By repeating this cycle a few times, and
by training the weights of the recurrent neural network to minimize
the loss function <span class="math notranslate nohighlight">\(y_t\)</span>, a good initialization heuristic is
found for the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> of the variational
quantum circuit.</p>
<p>At a given iteration, the RNN receives as input the previous cost
function <span class="math notranslate nohighlight">\(y_t\)</span> evaluated on the quantum computer, where
<span class="math notranslate nohighlight">\(y_t\)</span> is the estimate of <span class="math notranslate nohighlight">\(\langle H\rangle_{t}\)</span>, as well as
the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_t\)</span> for which the variational
circuit was evaluated. The RNN at this time step also receives
information stored in its internal hidden state from the previous time
step <span class="math notranslate nohighlight">\(\boldsymbol{h}_t\)</span>. The RNN itself has trainable parameters <span class="math notranslate nohighlight">\(\phi\)</span>,
and hence it applies the parametrized mapping:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{h}_{t+1}, \boldsymbol{\theta}_{t+1} = \text{RNN}_{\phi}(\boldsymbol{h}_{t}, \boldsymbol{\theta}_{t}, y_{t}),\]</div>
<p>which generates a new suggestion for the variational parameters as well
as a new internal state.
Upon training the weights <span class="math notranslate nohighlight">\(\phi\)</span>, the RNN
eventually learns a good heuristic to suggest optimal parameters for the
quantum circuit.</p>
<p>Thus, by training on a dataset of graphs, the RNN can subsequently be used to
provide suggestions for starting points on new graphs! We are not directly optimizing the variational parameters of
the quantum circuit, but instead, we let the RNN figure out how to do that.
In this sense, we are learning (training the RNN) how to learn (how to optimize a variational quantum circuit).</p>
<p><strong>VQAs in focus: QAOA for MaxCut</strong></p>
<p>There are multiple VQAs for which this hybrid training routine could
be used, some of them directly analyzed in <a class="footnote-reference" href="#l2l" id="id6">[1]</a>. In the
following, we focus on one such example, the
Quantum Approximate Optimization Algorithm (QAOA) for solving
the MaxCut problem <a class="footnote-reference" href="#maxcutwiki" id="id7">[4]</a>. Thus, referring to the picture above,
the shape of the variational circuit is the one dictated by the QAOA
ansatz, and such a quantum circuit is used to evaluate the cost
Hamiltonian <span class="math notranslate nohighlight">\(H\)</span> of the MaxCut problem.
Check out this great tutorial on
how to use QAOA for solving graph problems: <a class="reference external" href="https://pennylane.ai/qml/demos/tutorial_qaoa_intro.html">https://pennylane.ai/qml/demos/tutorial_qaoa_intro.html</a></p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Running the tutorial (excluding the Appendix) requires approx. ~13m.</p>
</div>
<p><strong>Importing the required packages</strong></p>
<p>During this tutorial, we will use
<strong>PennyLane</strong> for executing quantum circuits and for integrating
seamlessly with <strong>TensorFlow</strong>, which will be used for creating the RNN.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Quantum Machine Learning</span>
<span class="kn">import</span> <span class="nn">pennylane</span> <span class="k">as</span> <span class="nn">qml</span>
<span class="kn">from</span> <span class="nn">pennylane</span> <span class="kn">import</span> <span class="n">qaoa</span>

<span class="c1"># Classical Machine Learning</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Generation of graphs</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>

<span class="c1"># Standard Python libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># Fix the seed for reproducibility, which affects all random functions in this demo</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="generation-of-training-data-graphs">
<h2>Generation of training data: graphs<a class="headerlink" href="#generation-of-training-data-graphs" title="Permalink to this headline">¶</a></h2>
<p>The first step is to gather or
create a good dataset that will be used to train the model
and test its performance. In our case, we are analyzing MaxCut,
which deals with the problem of finding a good binary partition
of nodes in a graph such that the number of edges <em>cut</em> by such a
separation is maximized. We start by generating some
random graphs <span class="math notranslate nohighlight">\(G_{n,p}\)</span> where:</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(n\)</span> is the number of nodes in each graph,</li>
<li><span class="math notranslate nohighlight">\(p\)</span> is the probability of having an edge between two nodes.</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_graphs</span><span class="p">(</span><span class="n">n_graphs</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">,</span> <span class="n">p_edge</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generate a list containing random graphs generated by Networkx.&quot;&quot;&quot;</span>

    <span class="n">datapoints</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_graphs</span><span class="p">):</span>
        <span class="n">random_graph</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">gnp_random_graph</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p_edge</span><span class="p">)</span>
        <span class="n">datapoints</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">random_graph</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">datapoints</span>
</pre></div>
</div>
<p>An example of a random graph generated using the function
<code class="docutils literal notranslate"><span class="pre">generate_graphs</span></code> just defined:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define parameters of the graphs</span>
<span class="n">n_graphs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">n_nodes</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">p_edge</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="o">/</span> <span class="n">n_nodes</span>
<span class="n">graphs</span> <span class="o">=</span> <span class="n">generate_graphs</span><span class="p">(</span><span class="n">n_graphs</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">,</span> <span class="n">p_edge</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">graphs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="javascript:void(0);"><img alt="../_images/rendered_Graph0.png" src="../_images/rendered_Graph0.png" style="width: 70%;" /></a>
</div>
</div>
<div class="section" id="variational-quantum-circuit-qaoa">
<h2>Variational Quantum Circuit: QAOA<a class="headerlink" href="#variational-quantum-circuit-qaoa" title="Permalink to this headline">¶</a></h2>
<p>Now that we have a dataset, we move on by creating the QAOA quantum
circuits using PennyLane’s built-in sub-packages. In particular, using
PennyLane’s <code class="docutils literal notranslate"><span class="pre">qaoa</span></code> module, we will able to create fully functioning
quantum circuits for the MaxCut problem, with very few lines of code.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">qaoa_from_graph</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Uses QAOA to create a cost Hamiltonian for the MaxCut problem.&quot;&quot;&quot;</span>

    <span class="c1"># Number of qubits (wires) equal to the number of nodes in the graph</span>
    <span class="n">wires</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">))</span>

    <span class="c1"># Define the structure of the cost and mixer subcircuits for the MaxCut problem</span>
    <span class="n">cost_h</span><span class="p">,</span> <span class="n">mixer_h</span> <span class="o">=</span> <span class="n">qaoa</span><span class="o">.</span><span class="n">maxcut</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>

    <span class="c1"># Defines a layer of the QAOA ansatz from the cost and mixer Hamiltonians</span>
    <span class="k">def</span> <span class="nf">qaoa_layer</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="n">qaoa</span><span class="o">.</span><span class="n">cost_layer</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">cost_h</span><span class="p">)</span>
        <span class="n">qaoa</span><span class="o">.</span><span class="n">mixer_layer</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">mixer_h</span><span class="p">)</span>

    <span class="c1"># Creates the actual quantum circuit for the QAOA algorithm</span>
    <span class="k">def</span> <span class="nf">circuit</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">wires</span><span class="p">:</span>
            <a href="https://pennylane.readthedocs.io/en/stable/code/api/pennylane.Hadamard.html#pennylane.Hadamard" title="pennylane.Hadamard" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qml</span><span class="o">.</span><span class="n">Hadamard</span></a><span class="p">(</span><span class="n">wires</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>
        <a href="https://pennylane.readthedocs.io/en/stable/code/api/pennylane.layer.html#pennylane.layer" title="pennylane.layer" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">layer</span></a><span class="p">(</span><span class="n">qaoa_layer</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <a href="https://pennylane.readthedocs.io/en/stable/code/api/pennylane.expval.html#pennylane.expval" title="pennylane.expval" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">expval</span></a><span class="p">(</span><span class="n">cost_h</span><span class="p">)</span>

    <span class="c1"># Evaluates the cost Hamiltonian</span>
    <span class="k">def</span> <span class="nf">hamiltonian</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Evaluate the cost Hamiltonian, given the angles and the graph.&quot;&quot;&quot;</span>

        <span class="c1"># We set the default.qubit.tf device for seamless integration with TensorFlow</span>
        <span class="n">dev</span> <span class="o">=</span> <a href="https://pennylane.readthedocs.io/en/stable/code/api/pennylane.device.html#pennylane.device" title="pennylane.device" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">&quot;default.qubit.tf&quot;</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">))</span>

        <span class="c1"># This qnode evaluates the expectation value of the cost hamiltonian operator</span>
        <span class="n">cost</span> <span class="o">=</span> <a href="https://pennylane.readthedocs.io/en/stable/code/api/pennylane.QNode.html#pennylane.QNode" title="pennylane.QNode" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qml</span><span class="o">.</span><span class="n">QNode</span></a><span class="p">(</span><span class="n">circuit</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">interface</span><span class="o">=</span><span class="s2">&quot;tf&quot;</span><span class="p">,</span> <span class="n">diff_method</span><span class="o">=</span><span class="s2">&quot;backprop&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">cost</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">hamiltonian</span>
</pre></div>
</div>
<p>Before continuing, let’s see how to use these functions.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create an instance of a QAOA circuit given a graph.</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">qaoa_from_graph</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">graphs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Since we use only one layer in QAOA, params have the shape 1 x 2,</span>
<span class="c1"># in the form [[alpha, gamma]].</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Evaluate th QAOA instance just created with some angles.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cost</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(1,), dtype=float64, numpy=array([-3.19326796])&gt;
</pre></div>
</div>
</div>
<div class="section" id="recurrent-neural-network-lstm">
<h2>Recurrent Neural Network: LSTM<a class="headerlink" href="#recurrent-neural-network-lstm" title="Permalink to this headline">¶</a></h2>
<p>So far, we have defined the machinery which lets us build the QAOA
algorithm for solving the MaxCut problem.
Now we wish to implement the Recurrent Neural Network architecture
explained previously. As proposed in the original
paper, we will build a custom model of a Long-Short Term
Memory (LSTM) network, capable of handling the hybrid data passing between
classical and quantum procedures. For this task, we will use <code class="docutils literal notranslate"><span class="pre">Keras</span></code>
and <code class="docutils literal notranslate"><span class="pre">TensorFlow</span></code>.</p>
<p>First of all, let’s define the elemental building block of the model,
an LSTM cell (see <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell">TensorFlow
documentation</a>
for further details).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the number of layers in the QAOA ansatz.</span>
<span class="c1"># The higher the better in terms of performance, but it also gets more</span>
<span class="c1"># computationally expensive. For simplicity, we stick to the single layer case.</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Define a single LSTM cell.</span>
<span class="c1"># The cell has two units per layer since each layer in the QAOA ansatz</span>
<span class="c1"># makes use of two parameters.</span>
<span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_layers</span><span class="p">)</span>
</pre></div>
</div>
<p>Using the <code class="docutils literal notranslate"><span class="pre">qaoa_from_graph</span></code> function, we create a list
<code class="docutils literal notranslate"><span class="pre">graph_cost_list</span></code> containing the cost functions of a set of graphs.
You can see this as a preprocessing step of the data.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># We create the QAOA MaxCut cost functions of some graphs</span>
<span class="n">graph_cost_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">qaoa_from_graph</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">graphs</span><span class="p">]</span>
</pre></div>
</div>
<p>At this stage, we seek to reproduce the recurrent behavior depicted in
the picture above, outlining the functioning of an RNN as a black-box
optimizer. We do so by defining two functions:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">rnn_iteration</span></code>: accounts for the computations happening on a single time step in the figure.
It performs the calculation inside the CPU and evaluates the quantum circuit on the QPU to obtain
the loss function for the current parameters.</li>
<li><code class="docutils literal notranslate"><span class="pre">recurrent_loop</span></code>: as the name suggests, it accounts for the creation of the recurrent loop
of the model. In particular, it makes consecutive calls to the <code class="docutils literal notranslate"><span class="pre">rnn_iteration</span></code> function,
where the outputs of a previous call are fed as inputs of the next call.</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rnn_iteration</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">graph_cost</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Perform a single time step in the computational graph of the custom RNN.&quot;&quot;&quot;</span>

    <span class="c1"># Unpack the input list containing the previous cost, parameters,</span>
    <span class="c1"># and hidden states (denoted as &#39;h&#39; and &#39;c&#39;).</span>
    <span class="n">prev_cost</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">prev_params</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">prev_h</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">prev_c</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

    <span class="c1"># Concatenate the previous parameters and previous cost to create new input</span>
    <span class="n">new_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">prev_cost</span><span class="p">,</span> <span class="n">prev_params</span><span class="p">])</span>

    <span class="c1"># Call the LSTM cell, which outputs new values for the parameters along</span>
    <span class="c1"># with new internal states h and c</span>
    <span class="n">new_params</span><span class="p">,</span> <span class="p">[</span><span class="n">new_h</span><span class="p">,</span> <span class="n">new_c</span><span class="p">]</span> <span class="o">=</span> <span class="n">cell</span><span class="p">(</span><span class="n">new_input</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="p">[</span><span class="n">prev_h</span><span class="p">,</span> <span class="n">prev_c</span><span class="p">])</span>

    <span class="c1"># Reshape the parameters to correctly match those expected by PennyLane</span>
    <span class="n">_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_params</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">))</span>

    <span class="c1"># Evaluate the cost using new angles</span>
    <span class="n">_cost</span> <span class="o">=</span> <span class="n">graph_cost</span><span class="p">(</span><span class="n">_params</span><span class="p">)</span>

    <span class="c1"># Reshape to be consistent with other tensors</span>
    <span class="n">new_cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">_cost</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">new_cost</span><span class="p">,</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_c</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">recurrent_loop</span><span class="p">(</span><span class="n">graph_cost</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">intermediate_steps</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates the recurrent loop for the Recurrent Neural Network.&quot;&quot;&quot;</span>

    <span class="c1"># Initialize starting all inputs (cost, parameters, hidden states) as zeros.</span>
    <span class="n">initial_cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">initial_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_layers</span><span class="p">))</span>
    <span class="n">initial_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_layers</span><span class="p">))</span>
    <span class="n">initial_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_layers</span><span class="p">))</span>

    <span class="c1"># We perform five consecutive calls to &#39;rnn_iteration&#39;, thus creating the</span>
    <span class="c1"># recurrent loop. More iterations lead to better results, at the cost of</span>
    <span class="c1"># more computationally intensive simulations.</span>
    <span class="n">out0</span> <span class="o">=</span> <span class="n">rnn_iteration</span><span class="p">([</span><span class="n">initial_cost</span><span class="p">,</span> <span class="n">initial_params</span><span class="p">,</span> <span class="n">initial_h</span><span class="p">,</span> <span class="n">initial_c</span><span class="p">],</span> <span class="n">graph_cost</span><span class="p">)</span>
    <span class="n">out1</span> <span class="o">=</span> <span class="n">rnn_iteration</span><span class="p">(</span><span class="n">out0</span><span class="p">,</span> <span class="n">graph_cost</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="n">rnn_iteration</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">graph_cost</span><span class="p">)</span>
    <span class="n">out3</span> <span class="o">=</span> <span class="n">rnn_iteration</span><span class="p">(</span><span class="n">out2</span><span class="p">,</span> <span class="n">graph_cost</span><span class="p">)</span>
    <span class="n">out4</span> <span class="o">=</span> <span class="n">rnn_iteration</span><span class="p">(</span><span class="n">out3</span><span class="p">,</span> <span class="n">graph_cost</span><span class="p">)</span>

    <span class="c1"># This cost function takes into account the cost from all iterations,</span>
    <span class="c1"># but using different weights.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
        <span class="p">[</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">out0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">out1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">out2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.4</span> <span class="o">*</span> <span class="n">out3</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">out4</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">intermediate_steps</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">out0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">out1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">out2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">out3</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">out4</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">loss</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p><strong>The cost function</strong></p>
<p>A key part in the <code class="docutils literal notranslate"><span class="pre">recurrent_loop</span></code> function is given by the
definition of the variable <code class="docutils literal notranslate"><span class="pre">loss</span></code>. In order to drive the learning
procedure of the weights in the LSTM cell, a cost function is needed.
While in the original paper the authors suggest using a measure called
<em>observed improvement</em>, for simplicity here we use an easier cost
function <span class="math notranslate nohighlight">\(\cal{L}(\phi)\)</span> defined as:</p>
<div class="math notranslate nohighlight">
\[\cal{L}(\phi) = {\bf w} \cdot {\bf y}_t(\phi),\]</div>
<p>where <span class="math notranslate nohighlight">\({\bf y}_t(\phi) = (y_1, \cdots, y_5)\)</span> contains the
Hamiltonian cost functions from all iterations, and <span class="math notranslate nohighlight">\({\bf w}\)</span> are
just some coefficients weighting the different steps in the recurrent
loop. In this case, we used <span class="math notranslate nohighlight">\({\bf w}=\frac{1}{5} (0.1, 0.2, 0.3, 0.4, 0.5)\)</span>,
to give more importance to the last steps rather than the initial steps.
Intuitively in this way the RNN is more free (low coefficient) to
explore a larger portion of parameter space during the first steps of
optimization, while it is constrained (high coefficient) to select an
optimal solution towards the end of the procedure. Note that one could
also use just the final cost function from the last iteration to drive
the training procedure of the RNN. However, using values also from
intermediate steps allows for a smoother suggestion routine, since even
non-optimal parameter suggestions from early steps are penalized using
<span class="math notranslate nohighlight">\(\cal{L}(\phi)\)</span>.</p>
<p><strong>Training</strong></p>
<p>Now all the cards are on the table and we just need to prepare a
training routine and then run it!</p>
<p>First of all, let’s wrap a single gradient descent step inside a custom
function <code class="docutils literal notranslate"><span class="pre">train_step</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">graph_cost</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Single optimization step in the training procedure.&quot;&quot;&quot;</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="c1"># Evaluates the cost function</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">recurrent_loop</span><span class="p">(</span><span class="n">graph_cost</span><span class="p">)</span>

    <span class="c1"># Evaluates gradients, cell is the LSTM cell defined previously</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">trainable_weights</span><span class="p">)</span>

    <span class="c1"># Apply gradients and update the weights of the LSTM cell</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">trainable_weights</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>We are now ready to start the training. In particular, we will perform a
stochastic gradient descent in the parameter space of the weights of the
LSTM cell. For each graph in the training set, we evaluate gradients and
update the weights accordingly. Then, we repeat this procedure for
multiple times (epochs).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Be careful when using bigger datasets or training for larger
epochs, this may take a while to execute.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select an optimizer</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Set the number of training epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">graph_cost</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">graph_cost_list</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">graph_cost</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_loss</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="c1"># Log every 5 batches.</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; &gt; Graph </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">graph_cost_list</span><span class="p">)</span><span class="si">}</span><span class="s2"> - Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; &gt;&gt; Mean Loss during epoch: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1
 &gt; Graph 1/20 - Loss: -1.6641689538955688
 &gt; Graph 6/20 - Loss: -1.4186843633651733
 &gt; Graph 11/20 - Loss: -1.3757232427597046
 &gt; Graph 16/20 - Loss: -1.294339656829834
 &gt;&gt; Mean Loss during epoch: -1.7352586269378663
Epoch 2
 &gt; Graph 1/20 - Loss: -2.119091749191284
 &gt; Graph 6/20 - Loss: -1.4789190292358398
 &gt; Graph 11/20 - Loss: -1.3779840469360352
 &gt; Graph 16/20 - Loss: -1.2963457107543945
 &gt;&gt; Mean Loss during epoch: -1.8252217948436738
Epoch 3
 &gt; Graph 1/20 - Loss: -2.1322619915008545
 &gt; Graph 6/20 - Loss: -1.459418535232544
 &gt; Graph 11/20 - Loss: -1.390620470046997
 &gt; Graph 16/20 - Loss: -1.3165746927261353
 &gt;&gt; Mean Loss during epoch: -1.8328069806098939
Epoch 4
 &gt; Graph 1/20 - Loss: -2.1432175636291504
 &gt; Graph 6/20 - Loss: -1.476362943649292
 &gt; Graph 11/20 - Loss: -1.3938289880752563
 &gt; Graph 16/20 - Loss: -1.3140206336975098
 &gt;&gt; Mean Loss during epoch: -1.8369774043560028
Epoch 5
 &gt; Graph 1/20 - Loss: -2.1429405212402344
 &gt; Graph 6/20 - Loss: -1.477513074874878
 &gt; Graph 11/20 - Loss: -1.3909202814102173
 &gt; Graph 16/20 - Loss: -1.315887689590454
 &gt;&gt; Mean Loss during epoch: -1.8371947884559632
</pre></div>
</div>
<p>As you can see, the Loss for each graph keeps decreasing across epochs,
indicating that the training routine is working correctly.</p>
</div>
<div class="section" id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<p>Let’s see how to use the optimized RNN as an initializer for the angles
in the QAOA algorithm.</p>
<p>First, we pick a new graph, not present in the training dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">new_graph</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">gnp_random_graph</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">3</span> <span class="o">/</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">new_cost</span> <span class="o">=</span> <span class="n">qaoa_from_graph</span><span class="p">(</span><span class="n">new_graph</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">new_graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="javascript:void(0);"><img alt="../_images/rendered_Graph1.png" src="../_images/rendered_Graph1.png" style="width: 70%;" /></a>
</div>
<p>Then we apply the trained RNN to this new graph, saving intermediate
results coming from all the recurrent iterations in the network.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply the RNN (be sure that training was performed)</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">recurrent_loop</span><span class="p">(</span><span class="n">new_cost</span><span class="p">,</span> <span class="n">intermediate_steps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Extract all angle suggestions</span>
<span class="n">start_zeros</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_layers</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">guess_0</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">guess_1</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">guess_2</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">guess_3</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">guess_4</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
<span class="n">final_loss</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>

<span class="c1"># Wrap them into a list</span>
<span class="n">guesses</span> <span class="o">=</span> <span class="p">[</span><span class="n">start_zeros</span><span class="p">,</span> <span class="n">guess_0</span><span class="p">,</span> <span class="n">guess_1</span><span class="p">,</span> <span class="n">guess_2</span><span class="p">,</span> <span class="n">guess_3</span><span class="p">,</span> <span class="n">guess_4</span><span class="p">]</span>

<span class="c1"># Losses from the hybrid LSTM model</span>
<span class="n">lstm_losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_cost</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">guess</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">)))</span> <span class="k">for</span> <span class="n">guess</span> <span class="ow">in</span> <span class="n">guesses</span><span class="p">]</span>
</pre></div>
</div>
<p><strong>Plot of the loss function</strong></p>
<p>We can plot these losses to see how well the RNN proposes new guesses for
the parameters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lstm_losses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;-.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;LSTM&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="javascript:void(0);"><img alt="../_images/rendered_LossLSTM.png" src="../_images/rendered_LossLSTM.png" style="width: 70%;" /></a>
</div>
<p>That’s remarkable! The RNN learned to propose new parameters such that
the MaxCut cost is minimized very rapidly: in just a few iterations the
loss reaches a minimum. Actually, it takes just a single step for the LSTM
to find a very good minimum. In fact, due to the recurrent loop, the loss
in each time step is directly dependent on the previous ones, with the first
iteration thus having a lot of influence on the loss function defined above.
Changing the loss function, for example giving less importance to initial
steps and just focusing on the last one, leads to different optimization
behaviors, but with the same final results.</p>
<p><strong>Comparison with standard Stochastic Gradient Descent (SGD)</strong></p>
<p>How well does this method compare with
standard optimization techniques, for example, leveraging Stochastic
Gradient Descent (SGD) to optimize the parameters in the QAOA?</p>
<p>Let’s check it out.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Parameters are randomly initialized</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># We set the optimizer to be a Stochastic Gradient Descent</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">step</span> <span class="o">=</span> <span class="mi">15</span>

<span class="c1"># Training process</span>
<span class="n">steps</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">sdg_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">new_cost</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sdg_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">_</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> - Loss = </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final cost function: </span><span class="si">{</span><span class="n">new_cost</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">Optimized angles: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Step 1 - Loss = [-4.1700805]
Step 2 - Loss = [-4.67503588]
Step 3 - Loss = [-5.09949909]
Step 4 - Loss = [-5.40388533]
Step 5 - Loss = [-5.59529203]
Step 6 - Loss = [-5.70495197]
Step 7 - Loss = [-5.7642561]
Step 8 - Loss = [-5.79533198]
Step 9 - Loss = [-5.81138752]
Step 10 - Loss = [-5.81966529]
Step 11 - Loss = [-5.82396722]
Step 12 - Loss = [-5.82624537]
Step 13 - Loss = [-5.82749126]
Step 14 - Loss = [-5.82820626]
Step 15 - Loss = [-5.82864379]
Final cost function: -5.828932361904984
Optimized angles: [[ 0.5865477 ]
 [-0.3228858]]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sdg_losses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lstm_losses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;-.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;LSTM&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="javascript:void(0);"><img alt="../_images/rendered_LossConfrontation.png" src="../_images/rendered_LossConfrontation.png" style="width: 70%;" /></a>
</div>
<p><em>Hurray!</em> 🎉🎉</p>
<p>As is clear from the picture, the RNN reaches a better minimum in
fewer iterations than the standard SGD.
Thus, as the authors suggest, the trained RNN can
be used for a few iterations at the start of the training procedure to
initialize the parameters of the quantum circuit close to an optimal
solution. Then, a standard optimizer like the SGD can be used to
fine-tune the proposed parameters and reach even better solutions.
While on this small scale example the benefits of using an LSTM to
initialize parameters may seem modest, on more complicated instances
and problems it can make a big difference, since, on random
initialization of the parameters, standard local optimizer may
encounter problems finding a good minimization direction (for further
details, see <a class="footnote-reference" href="#l2l" id="id8">[1]</a>, <a class="footnote-reference" href="#vqas" id="id9">[2]</a>).</p>
</div>
<div class="section" id="final-remarks">
<h2>Final remarks<a class="headerlink" href="#final-remarks" title="Permalink to this headline">¶</a></h2>
<p>In this demo, we saw how to use a recurrent neural network
as a black-box optimizer to initialize the parameters in
a variational quantum circuit close to an optimal solution.
We connected MaxCut QAOA quantum circuits in PennyLane
with an LSTM built with TensorFlow, and we used a custom hybrid training
routine to optimize the whole network.</p>
<p>Such architecture proved itself to be a good candidate for the
initialization problem of Variational Quantum Algorithms, since it
reaches good optimal solutions in very few iterations. Besides, the
architecture is quite general since the same machinery can be used for
graphs having a generic number of nodes (see “Generalization Performances”
in the Appendix).</p>
<p><strong>What’s next?</strong></p>
<p>But the story does not end here. There are multiple ways this work could
be improved. Here are a few:</p>
<ul class="simple">
<li>Use the proposed architecture for VQAs other than QAOA for MaxCut.
You can check the paper <a class="footnote-reference" href="#l2l" id="id10">[1]</a> to get some inspiration.</li>
<li>Scale up the simulation, using bigger graphs and longer recurrent
loops.</li>
<li>While working correctly, the training routine is quite basic and it
could be improved for example by implementing batch learning or a
stopping criterion. Also, one could implement the
<em>observed improvement</em> loss function, as used in the original paper
<a class="footnote-reference" href="#l2l" id="id11">[1]</a>.</li>
<li>Depending on the problem, you may wish to transform the functions
<code class="docutils literal notranslate"><span class="pre">rnn_iteration</span></code> and <code class="docutils literal notranslate"><span class="pre">recurrent_loop</span></code> to actual <code class="docutils literal notranslate"><span class="pre">Keras</span> <span class="pre">Layers</span></code>
and <code class="docutils literal notranslate"><span class="pre">Models</span></code>. This way, by compiling the model before the training
takes place, <code class="docutils literal notranslate"><span class="pre">TensorFlow</span></code> can create the computational graph of the
model and train more efficiently. You can find
some ideas below to start working on it.</li>
</ul>
<p>If you’re interested, in the Appendix below you can find some more details
and insights about this model. Go check it out!</p>
<p>If you have any doubt, or wish to discuss about the project don’t
hesitate to contact me, I’ll be very happy to help you as much as I can
😁</p>
<p>Have a great quantum day!</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="l2l" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id5">2</a>, <a class="fn-backref" href="#id6">3</a>, <a class="fn-backref" href="#id8">4</a>, <a class="fn-backref" href="#id10">5</a>, <a class="fn-backref" href="#id11">6</a>)</em> Verdon G., Broughton M., McClean J. R., Sung K. J., Babbush R.,
Jiang Z., Neven H. and Mohseni M. “Learning to learn with quantum neural networks via classical neural
networks”, <a class="reference external" href="https://arxiv.org/abs/1907.05415">arXiv:1907.05415</a> (2019).</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="vqas" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><em>(<a class="fn-backref" href="#id2">1</a>, <a class="fn-backref" href="#id4">2</a>, <a class="fn-backref" href="#id9">3</a>)</em> Cerezo M., Arrasmith A., Babbush R., Benjamin S. C., Endo S.,
Fujii K., McClean J. R., Mitarai K., Yuan X., Cincio L. and Coles P.
J. “Variational Quantum Algorithms”, <a class="reference external" href="https://arxiv.org/abs/2012.09265">arXiv:2012.09265</a> (2020).</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="barren" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[3]</a></td><td>McClean J.R., Boixo S., Smelyanskiy V.N. et al.
“Barren plateaus in quantum neural network training landscapes”,
<a class="reference external" href="https://www.nature.com/articles/s41467-018-07090-4">Nat Commun 9, 4812</a> (2018).</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="maxcutwiki" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id7">[4]</a></td><td>MaxCut problem: <a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_cut">https://en.wikipedia.org/wiki/Maximum_cut</a>.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">¶</a></h2>
<p>In this appendix you can find further details about the Learning to Learn approach
introduced in this tutorial.</p>
<div class="section" id="generalization-performances">
<h3>Generalization performances<a class="headerlink" href="#generalization-performances" title="Permalink to this headline">¶</a></h3>
<p>A very interesting feature of this model, is that it can be
straightforwardly applied to graphs having a different number of nodes. In
fact, until now our analysis focused only on graphs with the same number
of nodes for ease of explanation, and there is no actual restriction in
this respect. The same machinery works fine for any graph, since the
number of QAOA parameters are only dependent on the number of layers in
the ansatz, and not on the number of qubits (equal to the number of
nodes in the graph) in the quantum circuit.</p>
<p>Thus, we might want to challenge our model to learn a good
initialization heuristic for a non-specific graph, with an arbitrary
number of nodes. For this purpose, let’s create a training dataset
containing graphs with a different number of nodes <span class="math notranslate nohighlight">\(n\)</span>, taken in
the interval <span class="math notranslate nohighlight">\(n \in [7,9]\)</span> (that is, our dataset now contains
graphs having either 7, 8 and 9 nodes).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_layers</span><span class="p">)</span>

<span class="n">g7</span> <span class="o">=</span> <span class="n">generate_graphs</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">g8</span> <span class="o">=</span> <span class="n">generate_graphs</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">g9</span> <span class="o">=</span> <span class="n">generate_graphs</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">7</span><span class="p">)</span>

<span class="n">gs</span> <span class="o">=</span> <span class="n">g7</span> <span class="o">+</span> <span class="n">g8</span> <span class="o">+</span> <span class="n">g9</span>
<span class="n">gs_cost_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">qaoa_from_graph</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gs</span><span class="p">]</span>

<span class="c1"># Shuffle the dataset</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">gs_cost_list</span><span class="p">)</span>
</pre></div>
</div>
<p>So far, we have created an equally balanced dataset that contains graphs with
a different number of nodes. We now use this dataset to train the LSTM.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select an optimizer</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Set the number of training epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">graph_cost</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gs_cost_list</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">graph_cost</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_loss</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="c1"># Log every 5 batches.</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; &gt; Graph </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">gs_cost_list</span><span class="p">)</span><span class="si">}</span><span class="s2"> - Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; &gt;&gt; Mean Loss during epoch: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1
&gt; Graph 1/15 - Loss: [[-1.4876363]]
&gt; Graph 6/15 - Loss: [[-1.8590403]]
&gt; Graph 11/15 - Loss: [[-1.7644017]]
&gt;&gt; Mean Loss during epoch: -1.9704322338104248
Epoch 2
&gt; Graph 1/15 - Loss: [[-1.8650053]]
&gt; Graph 6/15 - Loss: [[-1.9578737]]
&gt; Graph 11/15 - Loss: [[-1.8377447]]
&gt;&gt; Mean Loss during epoch: -2.092947308222453
Epoch 3
&gt; Graph 1/15 - Loss: [[-1.9009062]]
&gt; Graph 6/15 - Loss: [[-1.9726204]]
&gt; Graph 11/15 - Loss: [[-1.8668792]]
&gt;&gt; Mean Loss during epoch: -2.1162660201390584
</pre></div>
</div>
<p>Let’s check if this hybrid model eventually learned a good heuristic to
propose new updates for the parameters in the QAOA ansatz of the MaxCut
problem.</p>
<p>For this reason, we consider a new graph. In particular, we can take a
graph with 10 nodes, which is something that the recurrent network has
not seen before.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">new_graph</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">gnp_random_graph</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">3</span> <span class="o">/</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">new_cost</span> <span class="o">=</span> <span class="n">qaoa_from_graph</span><span class="p">(</span><span class="n">new_graph</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">new_graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="javascript:void(0);"><img alt="../_images/rendered_Graph10.png" src="../_images/rendered_Graph10.png" style="width: 70%;" /></a>
</div>
<p>We call the trained recurrent LSTM on this graph, saving not only the
last, but all intermediate guesses for the parameters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">recurrent_loop</span><span class="p">(</span><span class="n">new_cost</span><span class="p">,</span> <span class="n">intermediate_steps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Extract all angle suggestions</span>
<span class="n">start_zeros</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_layers</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">guess_0</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">guess_1</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">guess_2</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">guess_3</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">guess_4</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
<span class="n">final_loss</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>

<span class="c1"># Wrap them into a list</span>
<span class="n">guesses</span> <span class="o">=</span> <span class="p">[</span><span class="n">start_zeros</span><span class="p">,</span> <span class="n">guess_0</span><span class="p">,</span> <span class="n">guess_1</span><span class="p">,</span> <span class="n">guess_2</span><span class="p">,</span> <span class="n">guess_3</span><span class="p">,</span> <span class="n">guess_4</span><span class="p">]</span>

<span class="c1"># Losses from the hybrid LSTM model</span>
<span class="n">lstm_losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_cost</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">guess</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">)))</span> <span class="k">for</span> <span class="n">guess</span> <span class="ow">in</span> <span class="n">guesses</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lstm_losses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;-.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;LSTM&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="javascript:void(0);"><img alt="../_images/rendered_LossGeneralization.png" src="../_images/rendered_LossGeneralization.png" style="width: 70%;" /></a>
</div>
<p>Again, we can confirm that the custom optimizer based on the LSTM quickly reaches a good
value of the loss function, and also achieve good generalization performances, since
it is able to initialize parameters also for graphs not present in the training set.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">To get the optimized weights of the LSTM use: <code class="docutils literal notranslate"><span class="pre">optimized_weights</span> <span class="pre">=</span> <span class="pre">cell.get_weights()</span></code>.
To set initial weights for the LSTM cell, use instead: <code class="docutils literal notranslate"><span class="pre">cell.set_weights(optimized_weights)</span></code>.</p>
</div>
</div>
<div class="section" id="loss-landscape-in-parameter-space">
<h3>Loss landscape in parameter space<a class="headerlink" href="#loss-landscape-in-parameter-space" title="Permalink to this headline">¶</a></h3>
<p>It may be interesting to plot the path suggested by the RNN in the space
of the parameters. Note that this is possible only if one layer is used
in the QAOA ansatz since in this case only two angles are needed and
they can be plotted on a 2D plane. Of course, if more layers are used,
you can always select a pair of them to reproduce a similar plot.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This cell takes approx. ~1m to run with an 11 by 11 grid</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate the cost function on a grid in parameter space</span>
<span class="n">dx</span> <span class="o">=</span> <span class="n">dy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">dz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">new_cost</span><span class="p">([[</span><span class="n">xx</span><span class="p">],</span> <span class="p">[</span><span class="n">yy</span><span class="p">]])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">yy</span> <span class="ow">in</span> <span class="n">dy</span> <span class="k">for</span> <span class="n">xx</span> <span class="ow">in</span> <span class="n">dx</span><span class="p">])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">dz</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>

<span class="c1"># Plot cost landscape</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>

<span class="c1"># Extract optimizer steps</span>
<span class="n">params_x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">res</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]))]</span>
<span class="n">params_y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">res</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]))]</span>

<span class="c1"># Plot steps</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">params_x</span><span class="p">,</span> <span class="n">params_y</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\alpha$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\gamma$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loss Landscape&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="javascript:void(0);"><img alt="../_images/rendered_LossLandscape.png" src="../_images/rendered_LossLandscape.png" style="width: 70%;" /></a>
</div>
</div>
<div class="section" id="ideas-for-creating-a-keras-layer-and-keras-model">
<h3>Ideas for creating a Keras Layer and Keras Model<a class="headerlink" href="#ideas-for-creating-a-keras-layer-and-keras-model" title="Permalink to this headline">¶</a></h3>
<p>Definition of a <code class="docutils literal notranslate"><span class="pre">Keras</span> <span class="pre">Layer</span></code> containing a single pass through the
LSTM and the Quantum Circuit. That’s equivalent to the function
<code class="docutils literal notranslate"><span class="pre">rnn_iteration</span></code> from before.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">QRNN</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">graph</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">QRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># p is the number of layers in the QAOA ansatz</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expectation</span> <span class="o">=</span> <span class="n">qaoa_from_graph</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qaoa_p</span> <span class="o">=</span> <span class="n">p</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">prev_cost</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">prev_params</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">prev_h</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">prev_c</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

        <span class="c1"># Concatenate the previous parameters and previous cost to create new input</span>
        <span class="n">new_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">prev_cost</span><span class="p">,</span> <span class="n">prev_params</span><span class="p">])</span>

        <span class="c1"># New parameters obtained by the LSTM cell, along with new internal states h and c</span>
        <span class="n">new_params</span><span class="p">,</span> <span class="p">[</span><span class="n">new_h</span><span class="p">,</span> <span class="n">new_c</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell</span><span class="p">(</span><span class="n">new_input</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="p">[</span><span class="n">prev_h</span><span class="p">,</span> <span class="n">prev_c</span><span class="p">])</span>

        <span class="c1"># This part is used to feed the parameters to the PennyLane function</span>
        <span class="n">_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_params</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qaoa_p</span><span class="p">))</span>

        <span class="c1"># Cost evaluation, and reshaping to be consistent with other Keras tensors</span>
        <span class="n">new_cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expectation</span><span class="p">(</span><span class="n">_params</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">new_cost</span><span class="p">,</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_c</span><span class="p">]</span>
</pre></div>
</div>
<p>Code for creating an actual <code class="docutils literal notranslate"><span class="pre">Keras</span> <span class="pre">Model</span></code> starting from the previous
layer definition.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">_graph</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">gnp_random_graph</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">3</span> <span class="o">/</span> <span class="mi">7</span><span class="p">)</span>

<span class="c1"># Instantiate the LSTM cells</span>
<span class="n">rnn0</span> <span class="o">=</span> <span class="n">QRNN</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">_graph</span><span class="p">)</span>

<span class="c1"># Create some input layers to feed the data</span>
<span class="n">inp_cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">inp_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="n">inp_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="n">inp_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>

<span class="c1"># Manually creating the recurrent loops. In this case just three iterations are used.</span>
<span class="n">out0</span> <span class="o">=</span> <span class="n">rnn0</span><span class="p">([</span><span class="n">inp_cost</span><span class="p">,</span> <span class="n">inp_params</span><span class="p">,</span> <span class="n">inp_h</span><span class="p">,</span> <span class="n">inp_c</span><span class="p">])</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">rnn0</span><span class="p">(</span><span class="n">out0</span><span class="p">)</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">rnn0</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>

<span class="c1"># Definition of a loss function driving the training of the LSTM</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">average</span><span class="p">([</span><span class="mf">0.15</span> <span class="o">*</span> <span class="n">out0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.35</span> <span class="o">*</span> <span class="n">out1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">out2</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Definition of a Keras Model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">inp_cost</span><span class="p">,</span> <span class="n">inp_params</span><span class="p">,</span> <span class="n">inp_h</span><span class="p">,</span> <span class="n">inp_c</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">out0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">out1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">out2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">loss</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;functional_1&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 1)]          0
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 2)]          0
__________________________________________________________________________________________________
input_3 (InputLayer)            [(None, 2)]          0
__________________________________________________________________________________________________
input_4 (InputLayer)            [(None, 2)]          0
__________________________________________________________________________________________________
qrnn (QRNN)                     [(1, 1), (None, 2),  48          input_1[0][0]
                                                                input_2[0][0]
                                                                input_3[0][0]
                                                                input_4[0][0]
                                                                qrnn[0][0]
                                                                qrnn[0][1]
                                                                qrnn[0][2]
                                                                qrnn[0][3]
                                                                qrnn[1][0]
                                                                qrnn[1][1]
                                                                qrnn[1][2]
                                                                qrnn[1][3]
__________________________________________________________________________________________________
tf.math.multiply (TFOpLambda)   (1, 1)               0           qrnn[0][0]
__________________________________________________________________________________________________
tf.math.multiply_1 (TFOpLambda) (1, 1)               0           qrnn[1][0]
__________________________________________________________________________________________________
tf.math.multiply_2 (TFOpLambda) (1, 1)               0           qrnn[2][0]
__________________________________________________________________________________________________
average_147 (Average)           (1, 1)               0           tf.math.multiply[0][0]
                                                                tf.math.multiply_1[0][0]
                                                                tf.math.multiply_2[0][0]
==================================================================================================
Total params: 48
Trainable params: 48
Non-trainable params: 0
</pre></div>
</div>
<p>A basic training routine for the <code class="docutils literal notranslate"><span class="pre">Keras</span> <span class="pre">Model</span></code> just created:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">inp_costA</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">inp_paramsA</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">))</span>
<span class="n">inp_hA</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">))</span>
<span class="n">inp_cA</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">))</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inp_costA</span><span class="p">,</span> <span class="n">inp_paramsA</span><span class="p">,</span> <span class="n">inp_hA</span><span class="p">,</span> <span class="n">inp_cA</span><span class="p">]</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">step</span> <span class="o">=</span> <span class="mi">5</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">_</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> - Loss = </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2"> - Cost = </span><span class="si">{</span><span class="n">qaoa_from_graph</span><span class="p">(</span><span class="n">_graph</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="n">p</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),(</span><span class="mi">2</span><span class="p">,</span> <span class="n">p</span><span class="p">)))</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Loss:&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Outs:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;out0&quot;</span><span class="p">,</span> <span class="s2">&quot;out1&quot;</span><span class="p">,</span> <span class="s2">&quot;out2&quot;</span><span class="p">,</span> <span class="s2">&quot;Loss&quot;</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; &gt;</span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">t</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Step 1 - Loss = [[-1.5563084]] - Cost = -4.762684301954701
Step 2 - Loss = [[-1.5649065]] - Cost = -4.799981173473755
Step 3 - Loss = [[-1.5741502]] - Cost = -4.840036354736862
Step 4 - Loss = [[-1.5841404]] - Cost = -4.883246647056216
Step 5 - Loss = [[-1.5948243]] - Cost = -4.929228976649736
Final Loss: [[-1.5948243]]
Final Outs:
&gt;out0: [[-0.01041588  0.01016874]]
&gt;out1: [[-0.04530389  0.38148248]]
&gt;out2: [[-0.10258182  0.4134117 ]]
&gt;Loss: [[-1.5948243]]
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This code works only for a single graph at a time, since a graph was
needed to create the <code class="docutils literal notranslate"><span class="pre">QRNN</span></code> <code class="docutils literal notranslate"><span class="pre">Keras</span> <span class="pre">Layer</span></code> named <code class="docutils literal notranslate"><span class="pre">rnn0</span></code>. Thus, in
order to actually train the RNN network for multiple graphs, the above
training routine must be modified. Otherwise, you could find a way to
define the model to accept as input a whole dataset of graphs, and not
just a single one. Still, this might prove particularly hard, since
TensorFlow deals with tensors, and is not able to directly manage
other data structures, like graphs or functions taking graphs as
input, like <code class="docutils literal notranslate"><span class="pre">qaoa_from_graph</span></code>.</p>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-demos-learning2learn-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<a class="reference download internal" download="" href="../_downloads/04fbc7682c20430114bc959290489d43/learning2learn.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">learning2learn.py</span></code></a></div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<a class="reference download internal" download="" href="../_downloads/20e5fece0da2298feba24fbb0834bb2d/learning2learn.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">learning2learn.ipynb</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>
</div>


              <div id="bottom-dl" class="xanadu-call-to-action-links">
                <div id="tutorial-type">demos/learning2learn</div>
                <div class="download-python-link">
                  <i class="fab fa-python"></i>&nbsp;
                  <div class="call-to-action-desktop-view">Download Python script</div>
                </div>
                <div class="download-notebook-link">
                  <i class="fas fa-download"></i>&nbsp;
                  <div class="call-to-action-desktop-view">Download Notebook</div>
                </div>
                <div class="github-view-link">
                  <i class="fab fa-github"></i>&nbsp;
                  <div class="call-to-action-desktop-view">View on GitHub</div>
                </div>
              </div>
            </div>
          </div>
              <div class="comment-container nano has-scrollbar">
  <div class="nano-content">
    
    <div id="comments">
      <h3>Contents</h3>
      <ul>
<li><a class="reference internal" href="#">Learning to learn with quantum neural networks</a><ul>
<li><a class="reference internal" href="#problem-optimization-of-variational-quantum-algorithms">Problem: Optimization of Variational Quantum Algorithms</a></li>
<li><a class="reference internal" href="#solution-classical-recurrent-neural-networks">Solution: Classical Recurrent Neural Networks</a></li>
<li><a class="reference internal" href="#generation-of-training-data-graphs">Generation of training data: graphs</a></li>
<li><a class="reference internal" href="#variational-quantum-circuit-qaoa">Variational Quantum Circuit: QAOA</a></li>
<li><a class="reference internal" href="#recurrent-neural-network-lstm">Recurrent Neural Network: LSTM</a></li>
<li><a class="reference internal" href="#results">Results</a></li>
<li><a class="reference internal" href="#final-remarks">Final remarks</a></li>
<li><a class="reference internal" href="#references">References</a></li>
<li><a class="reference internal" href="#appendix">Appendix</a><ul>
<li><a class="reference internal" href="#generalization-performances">Generalization performances</a></li>
<li><a class="reference internal" href="#loss-landscape-in-parameter-space">Loss landscape in parameter space</a></li>
<li><a class="reference internal" href="#ideas-for-creating-a-keras-layer-and-keras-model">Ideas for creating a Keras Layer and Keras Model</a></li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>
    
    <div class="xanadu-call-to-action-links">
      <h3>Downloads</h3>
      <div id="tutorial-type">demos/learning2learn</div>
      <div class="download-python-link">
        <i class="fab fa-python"></i>&nbsp;
        <div class="call-to-action-desktop-view">Download Python script</div>
      </div>
      <div class="download-notebook-link">
        <i class="fas fa-download"></i>&nbsp;
        <div class="call-to-action-desktop-view">Download Notebook</div>
      </div>
      <div class="github-view-link">
        <i class="fab fa-github"></i>&nbsp;
        <div class="call-to-action-desktop-view">View on GitHub</div>
      </div>
      <div id="related-tutorials" class="mt-4">
      <h3> Related tutorials</h3>
      </div>
    </div>
  </div>
</div>
            

          <div class="up-button">
            
              
                <a href="../demos_qml.html"><i class="fas fa-angle-double-left"></i></a>
              
            
          </div>

          <div class="clearfix"></div>
        </div>


    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="tutorial_quanvolution.html" title="Quanvolutional Neural Networks"
             >next</a> |</li>
        <li class="right" >
          <a href="tutorial_qgrnn.html" title="The Quantum Graph Recurrent Neural Network"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">PennyLane  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../demonstrations.html" >Demos</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../demos_qml.html" >Quantum machine learning</a> &#187;</li> 
      </ul>
    </div>

    <!-- JQuery -->
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <!-- Bootstrap core JavaScript -->
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/js/bootstrap.min.js"></script>
    <!-- MDB core JavaScript -->
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.8.10/js/mdb.min.js"></script>
    <!-- Nanoscroller -->
    <script type="text/javascript" src="../_static/js/nanoscroller.min.js"></script>
    <script type="text/javascript">
        $('a.reference.external').each(function(){
          var link = $(this).attr("href");
          var hash = link.split('#')[1];
          var page = link.split('#')[0].split('/').slice(-1)[0].replace(".html", "");
          if (hash == page) {
            $(this).attr('href', link.split('#')[0]);
          }
        });
        $(".document > .section").removeClass("section");
        var tocContainer = document.querySelector('.comment-container');
        tocContainer.style.height = '85vh';
        $(".nano").nanoScroller();
    </script>
    <!-- lightslider -->
    <script src="../_static/js/lightslider.min.js"></script>

    <script type="text/javascript">
      $(window).scroll(function(){
          var windowHeight = window.innerHeight;
          var footer = document.querySelector('.page-footer');
          var footerPosition = footer.getBoundingClientRect();
          var tocContainer = document.querySelector('.comment-container');

          // Check if the footer is visible
          if (footerPosition.top < windowHeight && footerPosition.bottom >= 0) {
              // We want the height of the TOC to be the height of the main content minus how much of the footer is visible.
              tocContainer.style.height = 'calc(85vh - 45px - ' + (windowHeight - footerPosition.top) + 'px)'
          } else {
            // When the user scrolls back to the top of the page after scrolling to the bottom of the page,
            // We want to reset the TOC container back to it's original height
            if (tocContainer.style.height !== '85vh') tocContainer.style.height = '85vh';
          }
      });
      $(document).ready(function () {
          $(".css-transitions-only-after-page-load").each(function (index, element) {
              setTimeout(function () { $(element).removeClass("css-transitions-only-after-page-load") }, 10);
          });
      });
    </script>

    <script type="text/javascript">
    var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
    if (downloadNote.length >= 1) {
      var tutorialUrlArray = $("#tutorial-type").text().split('/');

      if (tutorialUrlArray[0] == "demos") {
        tutorialUrlArray[0] = "demonstrations";
      }

      var githubLink = "https://github.com/" + "XanaduAI/qml" + "/blob/master/" + tutorialUrlArray.join("/") + ".py",
          pythonLink = $(".sphx-glr-download .reference.download")[0].href,
          notebookLink = $(".sphx-glr-download .reference.download")[1].href,
          notebookDownloadPath = notebookLink.split('_downloads')[1].split('/').pop();

      $(".download-python-link").wrap("<a href=" + pythonLink + " data-behavior='call-to-action-event' data-response='Download Python script' download target='_blank'/>");
      $(".download-notebook-link").wrap("<a href=" + notebookLink + " data-behavior='call-to-action-event' data-response='Download Notebook' download target='_blank'/>");
      $(".github-view-link").wrap("<a href=" + githubLink + " data-behavior='call-to-action-event' data-response='View on Github' target='_blank'/>");
      $("#right-column").addClass("page-shadow");
    } else {
      $(".xanadu-call-to-action-links").hide();
      $("#bottom-dl").attr('style','display: none !important');
    }
    </script>

    <script type="text/javascript">
      function makeUL(urls, text) {
          var list = document.createElement('ul');

          for (var i = 0; i < urls.length; i++) {
              var item = document.createElement('li');
              var a = document.createElement('a');
              var linkText = document.createTextNode(text[i]);
              a.appendChild(linkText);
              a.href = urls[i];
              item.appendChild(a);
              list.appendChild(item);
          }
          return list;
      }

      if (typeof related_tutorials !== 'undefined') {
          document.getElementById('related-tutorials').appendChild(makeUL(related_tutorials, related_tutorials_titles));
          $("#related-tutorials ul li a").append(' <i class="fas fa-angle-double-right" style="font-size: smaller;"></i>')
          $("#related-tutorials").show();
      }
    </script>


    <script type="text/javascript">
        $(document).ready(function() {
            $("#featured-demos").lightSlider({
                item: 3,
                autoWidth: false,
                slideMove: 1, // slidemove will be 1 if loop is true
                slideMargin: 0,
                auto: true,
                loop: true,
                controls: true,
                pause: 5000,
                pager: false,
                prevHtml: "<i class='fas fa-chevron-left black-text' style='font-size: xx-large;'></i>",
                nextHtml: "<i class='fas fa-chevron-right black-text' style='font-size: xx-large;'></i>",
                responsive : [
                    {
                        breakpoint:1400,
                        settings: {
                            item:2,
                            slideMove:1,
                            slideMargin:0,
                          }
                    },
                    {
                        breakpoint:768,
                        settings: {
                            item:1,
                            slideMove:1,
                            slideMargin:6,
                          }
                    }
                ]
            });
        });
    </script>


  <footer class="page-footer text-md-left pt-4">
  
    <hr class="pb-0 mb-0">
    <div class="container-fluid">
      <div class="row   justify-content-md-center">
        <div class="col-md-3">
          <h5 class=" mb-1 footer-heading">Xanadu</h5>
          <hr width=100px class="d-inline-block mt-0 mb-1 Deep-purple accent-4">
          <p class="">Located in the heart of downtown Toronto, we've brought together exceptional minds from around the world to build quantum computers that are useful and available to people everywhere.</p>
        </div>

    <div class="col-md-2 col-4">
          <h5 class=" mb-1 footer-heading">PennyLane</h5>
          <hr width=100px class="d-inline-block mt-0 mb-1 Deep-purple accent-4">
          <ul class="list-unstyled">
            <li><a class="" href="https://pennylane.ai/">Home page</a></li>
            <li><a class="" href="https://github.com/XanaduAI/pennylane">GitHub</a></li>
            <li><a class="" href="https://pennylane.readthedocs.io/">Documentation</a></li>
            <li><a class="" href="https://discuss.pennylane.ai/">Discussion forum</a></li>
            <li><a class="" href="https://twitter.com/pennylaneai/">Twitter</a></li>
          </ul>
        </div>
		<div class="col-md-2 col-4">
          <h5 class=" mb-1 footer-heading">Strawberry Fields</h5>
          <hr width=100px class="d-inline-block mt-0 mb-1 Deep-purple accent-4">
          <ul class="list-unstyled">
            <li><a class="" href="https://strawberryfields.ai/">Interactive</a></li>
            <li><a class="" href="https://github.com/XanaduAI/strawberryfields">GitHub</a></li>
            <li><a class="" href="https://strawberryfields.readthedocs.io/">Documentation</a></li>
            <li><a class="" href="https://u.strawberryfields.ai/slack/">Slack channel</a></li>
          </ul>
        </div>


        <div class="col-md-2 col-4">
          <h5 class=" mb-1 footer-heading">About</h5>
          <hr width=100px class="d-inline-block mt-0 mb-1 Deep-purple accent-4">
          <ul class="list-unstyled">
            <li><a class="" href="https://www.xanadu.ai/">Home</a></li>
            <li><a class="" href="https://www.xanadu.ai/hardware/">Hardware</a></li>
            <li><a class="" href="https://www.xanadu.ai/software/">Software</a></li>
            <li><a class="" href="https://www.xanadu.ai/research">Research</a></li>
            <li><a class="" href="https://medium.com/XanaduAI">Blog</a></li>
            <li><a class="" href="https://www.xanadu.ai/about/">About</a></li>
          </ul>
        </div>
      </div>
    </div>
    <hr>

    <!-- <hr class="pb-0 mb-0"> -->

    <!--Social buttons-->
    <div class="social-section text-center">
        <ul class="list-unstyled list-inline mb-0">
            <li class="list-inline-item"><a class="btn-fb" href="https://www.facebook.com/Xanadu-1312050742230493/"><i class="fab fa-facebook-f"> </i></a></li>
            <li class="list-inline-item"><a class="btn-tw" href="https://twitter.com/xanaduai"><i class="fab fa-twitter"> </i></a></li>
            <li class="list-inline-item"><a class="" href="https://medium.com/xanaduai"><i class="fab fa-medium-m"> </i></a></li>
            <li class="list-inline-item"><a class="btn-li" href="https://www.linkedin.com/company/xanaduai/"><i class="fab fa-linkedin-in"> </i></a></li>
            <li class="list-inline-item"><a class="btn-git" href="https://github.com/XanaduAI"><i class="fab fa-github"> </i></a></li>
        </ul>
        <a href="https://xanadu.us17.list-manage.com/subscribe?u=725f07a1d1a4337416c3129fd&id=294b062630" style="font-size: initial;">Stay updated with our newsletter</a>
    </div>
    <!--/.Social buttons-->

    <!--Copyright-->
    <div class="footer-copyright py-3 mt-0 text-center">
        <div class="container-fluid">
            © Copyright 2019 | Xanadu | All rights reserved
            <br>
             TensorFlow, the TensorFlow logo and any related marks are trademarks of Google Inc. 
        </div>
    </div>
  </footer>
  </body>
</html>